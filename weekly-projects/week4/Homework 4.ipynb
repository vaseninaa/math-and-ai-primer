{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3370b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Homework 4 (due 07/24/2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58735d16-cb06-47ba-ac58-c608b9d06f78",
   "metadata": {},
   "source": [
    "# Decision trees, interpretability, and algorithmic bias\n",
    "\n",
    "## Objective\n",
    "\n",
    "In this week's project, you will explore the COMPAS data set. COMPAS stands for \"Correctional Offender Management Profiling for Alternative Sanctions\". It is a software/algorithm that is used to assess the risk of a registered offender is going to commit another offense. Although researchers and journalists have pointed to [various problems](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing) of this algorithm over many years, the algorithm is still used to inform sentences and parole decisions in several US states. \n",
    "You can learn more about the COMPAS data set [here](https://www.propublica.org/datastore/dataset/compas-recidivism-risk-score-data-and-analysis). \n",
    "\n",
    "Through this project, you will practice fitting and validating several classification models and you will explore some distinct benefits of using decision trees in machine learning. As part of that exploration, you are going to audit your model for demographic biases via a \"closed box\" and an \"open box\" approach.\n",
    "\n",
    "The COMPAS data set is a favorite example among critics of machine learning because it demonstrates several shortcomings and failure modes of machine learning techniques. The lessons learned from this project might be discouraging, and they are important. Keep in mind, however, that what you see here does not generalize to all data sets. \n",
    "\n",
    "This project has four parts.\n",
    "\n",
    "### Part 1: Prepare the COMPAS data set  (PARTIALLY YOU TO COMPLETE)\n",
    "\n",
    "In this part, you will load the COMPAS data set, explore its content, and select several variables as features (i.e., queries) or class labels (i.e., responses). Some of these features are not numerical, so you will need to replace some categorical values with zeros and ones. Your features will include categorical variable with more than two categories. You will uses 1-hot encoding to include this feature in your data set. \n",
    "\n",
    "This part includes four steps:\n",
    "1. Load and explore data set\n",
    "2. Select features and response variables\n",
    "3. Construct numerical coding for categorical features\n",
    "4. Split the data\n",
    "\n",
    "### Part 2: Train and validate a decision tree  (PARTIALLY YOU TO COMPLETE)\n",
    "\n",
    "In this part, you will fit a decision tree to your data. You will examine the effect of tuning the complexity of the tree via the \"maximum number of leaves\" parameter and use 5-fold cross-validation to find an optimal value.\n",
    "\n",
    "This part includes three steps:\n",
    "\n",
    "1. Fit a decision tree on the training data\n",
    "2. Tune the parameter \"maximum number of leaves\"\n",
    "3. Calculate the selected model's test performance\n",
    "\n",
    "\n",
    "### Part 3: Auditing a decision tree for demographic biases  (PARTIALLY YOU TO COMPLETE)\n",
    "\n",
    "Your training data includes several demographic variables (i.e., age, sex, race). A crude way to assess whether a model has some demographic bias is to remove the corresponding variables from your training data and explore how that removal affects your model's performance. Decision trees have the advantage of being interpretable machine learning models. By going through the decision nodes (i.e., branching points), you can \"open the black box and look inside\". Specifically, you can assess how each feature is used in the decision making process.\n",
    "\n",
    "This part includes three steps:\n",
    "\n",
    "1. Fit a decision tree\n",
    "2. Check for racial bias via performance assessment\n",
    "3. Check for racial bias via decision rules\n",
    "\n",
    "### Part 4: Comparison to other linear classifiers (FOR YOU TO COMPLETE)\n",
    "\n",
    "For some types of data, decision trees tend to achieve lower prediction accuracies In this part, you will train and tune several classifiers on the COMPAS data. You will then compare their performance on your test set.\n",
    "\n",
    "This part includes three steps:\n",
    "\n",
    "1. Fit LDA and logistic regression\n",
    "2. Tune and fit ensemble methods\n",
    "3. Tune and fit SVC\n",
    "4. Compare performance metrics for all models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf61d6ee-6086-420a-a1b3-9002b2d292b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier, export_text\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b14b03-f395-4bb0-abce-f67293c4a5db",
   "metadata": {},
   "source": [
    "## Part 1: Prepare the COMPAS data set\n",
    "\n",
    ">In this part, you will load the COMPAS data set, explore its content, and select several variables as features (i.e., queries) or class labels (i.e., responses). Some of these features are not numerical, so you will need to replace some categorical values with zeros and ones. Your features will include categorical variable with more than two categories. You will uses 1-hot encoding to include this feature in your data set.\n",
    ">\n",
    ">This part includes four steps:\n",
    ">1. Load and explore data set\n",
    ">2. Select features and response variables\n",
    ">3. Construct numerical coding for categorical features\n",
    ">4. Split the data\n",
    "\n",
    "\n",
    "\n",
    "### Part 1, Step 1: Load and explore data set\n",
    "\n",
    "This folder includes the 'compas-scores-two-years.csv' file. The COMPAS data that you will use for this project is in this file. It is always a good idea to look at the raw data before proceeding with one's machine learning pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f35e4e7-8eab-44b2-a03f-5ed100f53516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "raw_data = pd.read_csv('compas-scores-two-years.csv')\n",
    "# print a list of variable names\n",
    "print(raw_data.columns)\n",
    "# look at the first 5 rows \n",
    "raw_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc0cb9b-f285-4da2-8c5e-bd3af43ee4d7",
   "metadata": {},
   "source": [
    "The data set includes 53 variables. There are different types of information. Some variables\n",
    "* personal data (e.g., name, first name (\"first\"), last name (\"last\")) \n",
    "* demographic data (i.e., sex, age, age category (\"age_cat\"), and race)\n",
    "* related to the person's history of commited offenses (e.g., juvenile felony count (\"juv_fel_count\"), juvenile misdemeanor count (\"juv_misd_count\"), and prior offenses count (\"priors-count\"))\n",
    "* related to the charge against the person (e.g., charge offense date (\"c_offense_date\"), charge arrest date (\"c_arrest_date\"), charge degree (\"c_charge_degree\"), and description of charge (\"c_charge_desc\"))\n",
    "* recidivism scores assigned by the COMPAS algorithm (e.g., \"decile_score\", \"score_text\", \"v_decile_score\", \"v_score_text\")\n",
    "* related to an actual recidivism charge (e.g., degree of recidivism charge (\"r_charge_degree\"), data of recidivism offense (\"r_offense_date\"), description of recidivism charge (\"r_charge_desc\"))\n",
    "* related to an actual violent recidivism charge (e.g., degree of violent recidivism charge (\"vr_charge_degree\"), data of violent recidivism offense (\"vr_offense_date\"), description of violent recidivism charge (\"vr_charge_desc\")).\n",
    "\n",
    "### Part 1, Step 2: Select features and response variables\n",
    "\n",
    "The ProPublica article was assessing bias in the COMPAS scores. Here, you will ignore the COMPAS scores and instead explore the challenges of predicting recidivism based on the survey data. What variables seem like sensible predictors? What variables would be sensible outcome variables? The code in the cell below selects some numerical and categorical variables for you to include in your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fcba38-1f99-4723-96d2-ddf46ac7f7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features and response variables\n",
    "\n",
    "# Features by type\n",
    "numerical_features = ['juv_misd_count', 'juv_other_count', 'juv_fel_count', \n",
    "    'priors_count', 'age']\n",
    "binary_categorical_features = ['sex', 'c_charge_degree']\n",
    "other_categorical_features = ['race']\n",
    "all_features = binary_categorical_features + other_categorical_features + numerical_features\n",
    "\n",
    "# Possible esponse variables\n",
    "response_variables = ['is_recid', 'is_violent_recid', 'two_year_recid']\n",
    "\n",
    "# Variables that are used for data cleaning\n",
    "check_variables = ['days_b_screening_arrest']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b0cbaa-0a7b-46bf-a6d6-25f7c13fc1c7",
   "metadata": {},
   "source": [
    "ProPublica filtered some observations (i.e., rows in the data frame). See their explanation below. Let's follow their procedure.\n",
    "\n",
    "\n",
    "> There are a number of reasons remove rows because of missing data:\n",
    ">\n",
    "> * If the charge date of a defendants Compas scored crime was not within 30 days from when the person was arrested, we assume that because of data quality reasons, that we do not have the right offense.\n",
    "> * We coded the recidivist flag -- is_recid -- to be -1 if we could not find a compas case at all.\n",
    "> * In a similar vein, ordinary traffic offenses -- those with a c_charge_degree of 'O' -- will not result in Jail time are removed (only two of them).\n",
    "> * We filtered the underlying data from Broward county to include only those rows representing people who had either recidivated in two years, or had at least two years outside of a correctional facility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11c07da-04e6-42be-b407-31569d56fbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subselect data\n",
    "df = raw_data[all_features+response_variables+check_variables]\n",
    "\n",
    "# Apply filters\n",
    "df = df[(df['days_b_screening_arrest'] <= 30) & \n",
    "        (df['days_b_screening_arrest'] >= -30) & \n",
    "        (df['is_recid'] != -1) & \n",
    "        (df['c_charge_degree'] != 'O')]\n",
    "\n",
    "df = df[all_features+response_variables]\n",
    "print('Dataframe has {} rows and {} columns.'.format(df.shape[0], df.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c24fa48-c751-4786-b0d6-54a25be38350",
   "metadata": {},
   "source": [
    "### Part 1, Step 3: Construct numerical coding for categorical features\n",
    "\n",
    "Some of these features in the subselected data are not numerical, so you will need to replace some categorical values with zeros and ones. Your features will include \"race\", which was surveyed as a one categorical variable with more than two categories. You will uses [1-hot encoding](https://en.wikipedia.org/wiki/One-hot) to include this feature in your data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fdd6d1-8ead-41ff-a9e6-6ce9149fc545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code binary features as 0 and 1\n",
    "for x in binary_categorical_features:\n",
    "    for new_value, value in enumerate(set(df[x])):\n",
    "        print(\"Replace {} with {}.\".format(value, new_value))\n",
    "        df = df.replace(value, new_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f026e54b-d977-4ee3-93b8-d47703af9213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 1-hot encoding for other categorical variables\n",
    "one_hot_features = []\n",
    "for x in other_categorical_features:\n",
    "    for new_feature, value in enumerate(set(df[x])):\n",
    "        feature_name = \"{}_is_{}\".format(x,value)\n",
    "        df.insert(3, feature_name, df[x]==value)\n",
    "        one_hot_features += [feature_name]\n",
    "\n",
    "# Check what the data frame looks like now\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731b3c0b-4322-4ec6-80c8-da10665dc72b",
   "metadata": {},
   "source": [
    "### Part 1, Step 4: Split the data\n",
    "\n",
    "Let's collect the features in one data frame and the responses in another data frame. After that, you will set a small portion of the data set aside for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ea639f-feff-4bf2-ae40-4bfb6714cf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of features\n",
    "features = numerical_features + binary_categorical_features + one_hot_features\n",
    "\n",
    "# features data frame\n",
    "X = df[features]\n",
    "\n",
    "# responses data frame\n",
    "Y = df[response_variables]\n",
    "\n",
    "# Split the data into a training set containing 90% of the data\n",
    "# and test set containing 10% of the data\n",
    "'''ADD SOME CODE HERE'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0556fa-cd30-439d-b671-80e165cfa5d2",
   "metadata": {},
   "source": [
    "# Part 2: Train and validate a decision tree\n",
    "\n",
    ">In this part, you will fit a decision tree to your data. You will examine the effect of tuning the complexity of the tree via the \"maximum number of leaves\" parameter and use 5-fold cross-validation to find an optimal value.\n",
    ">\n",
    ">This part includes three steps:\n",
    ">\n",
    ">1. Fit a decision tree on the training data\n",
    ">2. Tune the parameter \"maximum number of leaves\"\n",
    ">3. Calculate the selected model's test performance\n",
    "\n",
    "### Part 2, Step 1: Fit a decision tree on the training data\n",
    "\n",
    "Start by fitting a decision tree to your training data. Assess its training accuracy and its size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c2b97d-2348-4e7b-b008-92b99665e9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model\n",
    "'''ADD SOME CODE HERE'''\n",
    "    \n",
    "# Fit model to training data\n",
    "'''ADD SOME CODE HERE'''\n",
    "\n",
    "# Evaluate training accuracy\n",
    "'''ADD SOME CODE HERE'''\n",
    "\n",
    "# Check size of decision tree\n",
    "num_leaves = dtc.get_n_leaves()\n",
    "\n",
    "# Report results\n",
    "print('Trained decision tree with {} leaves and training accuracy {:.2f}.'.format(num_leaves, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881e0628-df55-49f3-a00a-123ca44baa98",
   "metadata": {},
   "source": [
    "Your tree has a good training accuracy for the standards of tabular data prediction problems, but its size is enormous! It has so many leaves, that on average every 3 to 4 training observations get a leaf to themselves. It is very probable that this tree is overfitting.\n",
    "\n",
    "### Part 2, Step 2: Tune the parameter \"maximum number of leaves\"\n",
    "\n",
    "Let's try to constrain the complexity of a decision tree during training by setting a value for the argument ``maximum number of leaves``. You can use the sci-kit learn's `cross_val_score` function to quickly assess the out-of-sample performance of trees of varying complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0668136-b649-4b42-be6b-e360ff7d2f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 5-fold cross-validation for different tree sizes\n",
    "\n",
    "print('Leaves\\tMean accuracy')\n",
    "print('---------------------')\n",
    "for num_leaves in range(100,1800,100):\n",
    "\n",
    "    # Trees must have at least 2 leaves\n",
    "    if num_leaves >= 2:\n",
    "\n",
    "        # construct a classifier with a limit on its number of leaves\n",
    "        '''ADD SOME CODE HERE'''\n",
    "\n",
    "        # Get validation accuracy via 5-fold cross-validation\n",
    "        scores = '''ADD SOME CODE HERE'''\n",
    "    \n",
    "    print(\"{}\\t{:.3f}\".format(num_leaves,scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a46824a-f7f5-4408-94fb-8c463c5a7e0f",
   "metadata": {},
   "source": [
    "Adjust the range of values for `max_leaf_nodes` in the cell above, to identify the best value.\n",
    "\n",
    "### Part 2, Step 3: Calculate the selected model's test performance\n",
    "\n",
    "Train a decision tree using your selected value of `max_leaf_nodes` on the full training set. Assess its accuracy on your test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502349cb-63ee-4146-8c5e-f58b26241165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model\n",
    "'''ADD SOME CODE HERE'''\n",
    "    \n",
    "# Fit model to training data\n",
    "'''ADD SOME CODE HERE'''\n",
    "\n",
    "# Evaluate training accuracy\n",
    "'''ADD SOME CODE HERE'''\n",
    "\n",
    "# Check size of decision tree\n",
    "'''ADD SOME CODE HERE'''\n",
    "\n",
    "# Report results\n",
    "print('Trained decision tree with {} leaves and test accuracy {:.2f}.'.format(num_leaves, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415c6b58-4f9e-4d22-b9c0-914b22aea0f9",
   "metadata": {},
   "source": [
    "# Part 3: Auditing a decision tree for demographic biases\n",
    "\n",
    ">Your training data includes several demographic variables (i.e., age, sex, race). A crude way to assess whether a model has some demographic bias is to remove the corresponding variables from your training data and explore how that removal affects your model's performance. Decision trees have the advantage of being interpretable machine learning models. By going through the decision nodes (i.e., branching points), you can \"open the black box and look inside\". Specifically, you can assess how each feature is used in the decision making process.\n",
    ">\n",
    ">This part includes two steps:\n",
    ">\n",
    ">1. Check for racial bias via performance assessment\n",
    ">2. Check for racial bias via decision rules\n",
    "  \n",
    "### Part 3, Step 2: Check for racial bias via performance assessment\n",
    "A simple approach to identifying demographic biases in machine learning is the following: (i) Train and validate the model on the full training set, (ii) train and validate the model on a subset of training variables that excludes the variables related to a potential demographic bias, (iii) compare the results. \n",
    "\n",
    "You have noticed that the validation accuracy of your model can vary for different holdout set selections. To account for these variations, you are going to compare the mean validation accuracy over 100 trees. (You have completed (i) in the previous cell already. Continue now with (ii).)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f5e4e1-5487-4220-afcc-eeb1e228c8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subset of training data without information on race. \n",
    "# (The information on race was encoded in the one-hot features.)\n",
    "remaining_features = [v for v in X.columns if v not in one_hot_features]\n",
    "X_train_sub = X_train[remaining_features]\n",
    "X_test_sub = X_test[remaining_features]\n",
    "\n",
    "# Create a model\n",
    "dtc = DecisionTreeClassifier(max_leaf_nodes=39)\n",
    "    \n",
    "# Fit model to training data\n",
    "dtc.fit(X_train_sub, y_train['two_year_recid'])\n",
    "\n",
    "# Evaluate training accuracy\n",
    "y_pred = dtc.predict(X_test_sub)\n",
    "accuracy = (y_pred == y_test['two_year_recid']).mean()\n",
    "\n",
    "# Check size of decision tree\n",
    "num_leaves = dtc.get_n_leaves()\n",
    "\n",
    "# Report results\n",
    "print('Trained decision tree with {} leaves and test accuracy {:.2f}.'.format(num_leaves, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e10fce-64ad-4f3c-95db-9e31a5e33568",
   "metadata": {},
   "source": [
    "Comparing the mean accuracy values on the all features versus the subselected feature set, what do you conclude about the importance of racial information in this classification problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18654ba4-834b-49d8-b110-712844d5bf62",
   "metadata": {},
   "source": [
    "### Part 2, Step 3: Check for racial bias via decision rules\n",
    "The interpretability of decision trees allows for an alternative approach to detecting racial bias. You can simply look at the decision rules. Use the scit-kit learn's function `export_text` to get your decision tree in text format. Compare the decision rules of the your tree with all features and your tree fitted on the subset without racial information. Do you find any indication of racial bias in the decision rules of the first tree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212a88fb-f48a-4081-87df-eb1ce5b69bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''ADD SOME CODE HERE'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e82dfe8-9fd7-44f9-aca4-8e33f9203798",
   "metadata": {},
   "source": [
    "# Part 4: Comparison to other linear classifiers\n",
    "\n",
    ">For some types of data, decision trees tend to achieve lower prediction accuracies In this part, you will train and tune several classifiers on the COMPAS data. You will then compare their performance on your test set.\n",
    ">\n",
    ">This part includes three steps:\n",
    ">\n",
    ">1. Fit LDA and logistic regression\n",
    ">2. Tune and fit ensemble methods\n",
    ">3. Tune and fit SVC\n",
    ">4. Compare test accuracy of all your models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97edaf2-04da-4f12-aa8f-75847bd6e3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''ADD SOME CODE HERE'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
